{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"About","date":"2025-08-11T20:07:15.961Z","updated":"2025-08-11T20:07:15.961Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2025-08-11T20:07:15.962Z","updated":"2025-08-11T20:07:15.961Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2025-08-11T20:07:15.962Z","updated":"2025-08-11T20:07:15.962Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"YOLOv3 model.py代码的简单分析注解","slug":"Yolo学习/YOLOv3","date":"2025-08-20T13:01:27.000Z","updated":"2025-08-24T20:20:50.000Z","comments":true,"path":"wiki/Yolo学习/YOLOv3/","permalink":"http://example.com/wiki/Yolo%E5%AD%A6%E4%B9%A0/YOLOv3/","excerpt":"","text":"YOLOv3 model.py代码的简单分析注解&gt; 准备12345678910from functools import wrapsimport numpy as npimport tensorflow as tffrom keras import backend as Kfrom keras.layers import Conv2D, Add, ZeroPadding2D, UpSampling2D, Concatenate, MaxPooling2Dfrom keras.layers.advanced_activations import LeakyReLUfrom keras.layers.normalization import BatchNormalizationfrom keras.models import Modelfrom keras.regularizers import l2from yolo3.utils import compose &gt; YOLO网络首先要通过特征提取网络对输入的输入图像提取特征，得到不同尺寸的特征图### 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@wraps(Conv2D)#@wraps在此处的作用是为了使DarknetConv2D.__name__和DarknetConv2D.__doc__与Conv2D__name__和Conv2D.__doc__相同。def DarknetConv2D(*args, **kwargs): &quot;&quot;&quot;Wrapper to set Darknet parameters for Convolution2D.&quot;&quot;&quot; darknet_conv_kwargs = &#123;&#x27;kernel_regularizer&#x27;: l2(5e-4)&#125; darknet_conv_kwargs[&#x27;padding&#x27;] = &#x27;valid&#x27; if kwargs.get(&#x27;strides&#x27;)==(2,2) else &#x27;same&#x27; darknet_conv_kwargs.update(kwargs) return Conv2D(*args, **darknet_conv_kwargs)def DarknetConv2D_BN_Leaky(*args, **kwargs): &quot;&quot;&quot;Darknet Convolution2D followed by BatchNormalization and LeakyReLU.&quot;&quot;&quot; no_bias_kwargs = &#123;&#x27;use_bias&#x27;: False&#125; no_bias_kwargs.update(kwargs) return compose( DarknetConv2D(*args, **no_bias_kwargs), BatchNormalization(), LeakyReLU(alpha=0.1))def resblock_body(x, num_filters, num_blocks): &#x27;&#x27;&#x27;A series of resblocks starting with a downsampling Convolution2D&#x27;&#x27;&#x27; # Darknet uses left and top padding instead of &#x27;same&#x27; mode x = ZeroPadding2D(((1,0),(1,0)))(x)#((top_pad,bottom_pad),(left_pad,right_pad)) 这里的ZeroPadding2D是为紧跟着的DBL服务的，使其尺寸正好缩小为原来的一半 x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x) for i in range(num_blocks): y = compose( DarknetConv2D_BN_Leaky(num_filters//2, (1,1)), DarknetConv2D_BN_Leaky(num_filters, (3,3)))(x) x = Add()([x,y]) return xdef darknet_body(x): &#x27;&#x27;&#x27;Darknent body having 52 Convolution2D layers&#x27;&#x27;&#x27; x = DarknetConv2D_BN_Leaky(32, (3,3))(x) x = resblock_body(x, 64, 1) x = resblock_body(x, 128, 2) x = resblock_body(x, 256, 8) x = resblock_body(x, 512, 8) x = resblock_body(x, 1024, 4) return xdef make_last_layers(x, num_filters, out_filters): &#x27;&#x27;&#x27;6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer&#x27;&#x27;&#x27; x = compose( DarknetConv2D_BN_Leaky(num_filters, (1,1)), DarknetConv2D_BN_Leaky(num_filters*2, (3,3)), DarknetConv2D_BN_Leaky(num_filters, (1,1)), DarknetConv2D_BN_Leaky(num_filters*2, (3,3)), DarknetConv2D_BN_Leaky(num_filters, (1,1)))(x) y = compose( DarknetConv2D_BN_Leaky(num_filters*2, (3,3)), DarknetConv2D(out_filters, (1,1)))(x) return x, ydef yolo_body(inputs, num_anchors, num_classes): &quot;&quot;&quot;Create YOLO_V3 model CNN body in Keras.&quot;&quot;&quot; darknet = Model(inputs, darknet_body(inputs)) #第一个特征层y1=(batch_size,13,13,3,85) x, y1 = make_last_layers(darknet.output, 512, num_anchors*(num_classes+5)) x = compose( DarknetConv2D_BN_Leaky(256, (1,1)), UpSampling2D(2))(x) x = Concatenate()([x,darknet.layers[152].output])#darknet.layers[152].output]#这里的[152]中的数字为对应层数 #第二个特征层y2=(batch_size,26,26,3,85) x, y2 = make_last_layers(x, 256, num_anchors*(num_classes+5)) x = compose( DarknetConv2D_BN_Leaky(128, (1,1)), UpSampling2D(2))(x) x = Concatenate()([x,darknet.layers[92].output]) #第三个特征层y3=(batch_size,52,52,3,85) x, y3 = make_last_layers(x, 128, num_anchors*(num_classes+5)) return Model(inputs, [y1,y2,y3]) 流程图 整个yolov3网络共252层&gt; tiny_yolo_body轻量版的YOLO,有速度快，占内存少等的优点，视情况选择使用。12345678910111213141516171819202122232425262728293031def tiny_yolo_body(inputs, num_anchors, num_classes): &#x27;&#x27;&#x27;Create Tiny YOLO_v3 model CNN body in keras.&#x27;&#x27;&#x27; x1 = compose( DarknetConv2D_BN_Leaky(16, (3,3)), MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=&#x27;same&#x27;), DarknetConv2D_BN_Leaky(32, (3,3)), MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=&#x27;same&#x27;), DarknetConv2D_BN_Leaky(64, (3,3)), MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=&#x27;same&#x27;), DarknetConv2D_BN_Leaky(128, (3,3)), MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=&#x27;same&#x27;), DarknetConv2D_BN_Leaky(256, (3,3)))(inputs) x2 = compose( MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=&#x27;same&#x27;), DarknetConv2D_BN_Leaky(512, (3,3)), MaxPooling2D(pool_size=(2,2), strides=(1,1), padding=&#x27;same&#x27;), DarknetConv2D_BN_Leaky(1024, (3,3)), DarknetConv2D_BN_Leaky(256, (1,1)))(x1) y1 = compose( DarknetConv2D_BN_Leaky(512, (3,3)), DarknetConv2D(num_anchors*(num_classes+5), (1,1)))(x2) x2 = compose( DarknetConv2D_BN_Leaky(128, (1,1)), UpSampling2D(2))(x2) y2 = compose( Concatenate(), DarknetConv2D_BN_Leaky(256, (3,3)), DarknetConv2D(num_anchors*(num_classes+5), (1,1)))([x2,x1]) return Model(inputs, [y1,y2]) &gt; 预测框的获得1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#将预测值的每个特征层调成真实值def yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False): &quot;&quot;&quot;Convert final layer features to bounding box parameters.&quot;&quot;&quot; num_anchors = len(anchors)#num_anchors=3 # Reshape to batch, height, width, num_anchors, box_params. anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2]) grid_shape = K.shape(feats)[1:3] # height, width grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]), [1, grid_shape[1], 1, 1])#将x在各个维度上重复n次，x为张量，n为与x维度数目相同的列表 grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]), [grid_shape[0], 1, 1, 1]) grid = K.concatenate([grid_x, grid_y])#从倒数第1个维度进行拼接,可视为得到每个单元格左上角的坐标 grid = K.cast(grid, K.dtype(feats)) feats = K.reshape( feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])#(batch_size,13,13,3,85) # Adjust preditions to each spatial grid point and anchor size. #box_xy对应框的中心点，box_wh对应框的宽和高 box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))#grid 为偏移 ，将x,y相对于featuremap尺寸进行了归一化 box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))#exp() 方法返回x的指数,ex。 box_confidence = K.sigmoid(feats[..., 4:5])#置信率 box_class_probs = K.sigmoid(feats[..., 5:])#80个类别 #在计算loss的时候返回以下参数 if calc_loss == True: return grid, feats, box_xy, box_wh return box_xy, box_wh, box_confidence, box_class_probs#对box进行调整，使其符合真实图片的样子def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape): &#x27;&#x27;&#x27;Get corrected boxes&#x27;&#x27;&#x27; box_yx = box_xy[..., ::-1] box_hw = box_wh[..., ::-1] input_shape = K.cast(input_shape, K.dtype(box_yx)) image_shape = K.cast(image_shape, K.dtype(box_yx)) #这里K.min取较小值，便于后续步骤调整适应image的尺寸，new是image等比缩小得来 new_shape = K.round(image_shape * K.min(input_shape/image_shape))#K.round()底层即为tf.round(),Bankers Rounding——四舍六入五取偶,如4.5——&gt;4;3.5——&gt;4 offset = (input_shape-new_shape)/2./input_shape scale = input_shape/new_shape box_yx = (box_yx - offset) * scale box_hw *= scale box_mins = box_yx - (box_hw / 2.) box_maxes = box_yx + (box_hw / 2.) boxes = K.concatenate([ box_mins[..., 0:1], # y_min box_mins[..., 1:2], # x_min box_maxes[..., 0:1], # y_max box_maxes[..., 1:2] # x_max ])#（x_min,y_min),（x_max,y_max)分别为左上，右下点的坐标 # Scale boxes back to original image shape. boxes *= K.concatenate([image_shape, image_shape]) return boxes#获取每个box和它的得分def yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape): &#x27;&#x27;&#x27;Process Conv layer output&#x27;&#x27;&#x27; # -1,13,13,3,2;-1,13,13,3,2;-1,13,13,3,1;-1,13,13,3,80; box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats, anchors, num_classes, input_shape) #将box_xy,box_wh调节成y_min,y_max,x_min,x_max boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape) boxes = K.reshape(boxes, [-1, 4]) box_scores = box_confidence * box_class_probs box_scores = K.reshape(box_scores, [-1, num_classes]) return boxes, box_scores 求得bx,by,bw,bh的公式 &gt; 寻找最佳的anchor box12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485def preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes): &#x27;&#x27;&#x27;Preprocess true boxes to training input format Parameters ---------- true_boxes: array, shape=(m, T, 5) Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape. input_shape: array-like, hw, multiples of 32 anchors: array, shape=(N, 2), wh num_classes: integer Returns ------- y_true: list of array, shape like yolo_outputs, xywh are reletive value &#x27;&#x27;&#x27; assert (true_boxes[..., 4]&lt;num_classes).all(), &#x27;class id must be less than num_classes&#x27; #assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常。 #.all（）测试沿给定轴的所有数组元素是否都计算为True。元素除了是 0、空、None、False 外都算 True num_layers = len(anchors)//3 # default setting # 先验框 # 678为116，90；156，198；373，326 # 345为30，61；62，45；59，119 # 012为10，13；16，30；33，23 anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] true_boxes = np.array(true_boxes, dtype=&#x27;float32&#x27;) input_shape = np.array(input_shape, dtype=&#x27;int32&#x27;) #将边框的两点坐标形式，转化成训练的中心加边长形式，并进行归一化 boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2 boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2] true_boxes[..., 0:2] = boxes_xy/input_shape[::-1] true_boxes[..., 2:4] = boxes_wh/input_shape[::-1] m = true_boxes.shape[0]#m张图片 grid_shapes = [input_shape//&#123;0:32, 1:16, 2:8&#125;[l] for l in range(num_layers)] #y_true的格式为（m,13,13,3,85),(m,26,26,3,85),(m,52,52,3,85) num_classes是一个one_hot编码 y_true = [np.zeros((m,grid_shapes[l][0],grid_shapes[l][1],len(anchor_mask[l]),5+num_classes), dtype=&#x27;float32&#x27;) for l in range(num_layers)] # Expand dim to apply broadcasting.增加维度是为了更好的计算 anchors = np.expand_dims(anchors, 0)#np.expand_dims(input, dim, name=None)函数,将维度加1 #anchor/2和取反则是为了计算iou 可以看作把anchor的中心移到坐标原点 anchor_maxes = anchors / 2. anchor_mins = -anchor_maxes #长度要大于0才有效 valid_mask = boxes_wh[..., 0]&gt;0 for b in range(m): # Discard zero rows. wh = boxes_wh[b, valid_mask[b]] if len(wh)==0: continue # Expand dim to apply broadcasting. wh = np.expand_dims(wh, -2) box_maxes = wh / 2. box_mins = -box_maxes #计算真实框和哪个先验框最契合、对于有效的边框、采用与anchor相似的处理方法 #INTERSECT（交集) 集合运算 intersect_mins = np.maximum(box_mins, anchor_mins) intersect_maxes = np.minimum(box_maxes, anchor_maxes) intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.) intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1] box_area = wh[..., 0] * wh[..., 1] anchor_area = anchors[..., 0] * anchors[..., 1] iou = intersect_area / (box_area + anchor_area - intersect_area) # Find best anchor for each true box best_anchor = np.argmax(iou, axis=-1)#np.argmax()返回张量沿指定维度最大值的引索 for t, n in enumerate(best_anchor):#enumerate()是python的内置函数，可遍历每个元素，组合为索引 元素，常在for循环中使用 for l in range(num_layers): if n in anchor_mask[l]: #np.floor()返回不大于输入参数的最大整数。（向下取整） i = np.floor(true_boxes[b,t,0]*grid_shapes[l][1]).astype(&#x27;int32&#x27;)#中心点x在grid中的对应位置 j = np.floor(true_boxes[b,t,1]*grid_shapes[l][0]).astype(&#x27;int32&#x27;) k = anchor_mask[l](n)#返回真实框对应最契合的先验框中 c = true_boxes[b,t, 4].astype(&#x27;int32&#x27;)#该框所对应的类别 #将真实框的信息放进y_true与之对应的特征层的相对应中心点中 y_true[l][b, j, i, k, 0:4] = true_boxes[b,t, 0:4] y_true[l][b, j, i, k, 4] = 1 y_true[l][b, j, i, k, 5+c] = 1 return y_true preprocess_true_boxes中涉及到的anchor box和ground truth经过&#x2F;2和取反等处理后，其相对关系应大概如图所示： &gt; LOSS值的计算（1）计算xy（物体中心坐标）的损失：xyloss&#x3D;bool*(2-areaPred)*bce（2）计算wh（anchor长宽回归值）的损失：whloss&#x3D;bool*(2-areaPred)*bce（3）计算置信度的损失：whloss&#x3D;bool*(2-areaPred)*bce（4）计算类别损失：置信度乘上多分类的交叉熵123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129#定义一个iou函数def box_iou(b1, b2): &#x27;&#x27;&#x27;Return iou tensor Parameters ---------- b1: tensor, shape=(i1,...,iN, 4), xywh b2: tensor, shape=(j, 4), xywh Returns ------- iou: tensor, shape=(i1,...,iN, j) &#x27;&#x27;&#x27; # Expand dim to apply broadcasting. b1 = K.expand_dims(b1, -2) b1_xy = b1[..., :2] b1_wh = b1[..., 2:4] b1_wh_half = b1_wh/2. b1_mins = b1_xy - b1_wh_half b1_maxes = b1_xy + b1_wh_half # Expand dim to apply broadcasting. b2 = K.expand_dims(b2, 0) b2_xy = b2[..., :2] b2_wh = b2[..., 2:4] b2_wh_half = b2_wh/2. b2_mins = b2_xy - b2_wh_half b2_maxes = b2_xy + b2_wh_half intersect_mins = K.maximum(b1_mins, b2_mins) intersect_maxes = K.minimum(b1_maxes, b2_maxes) intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.) intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1] b1_area = b1_wh[..., 0] * b1_wh[..., 1] b2_area = b2_wh[..., 0] * b2_wh[..., 1] iou = intersect_area / (b1_area + b2_area - intersect_area) return ioudef yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False): &#x27;&#x27;&#x27;Return yolo_loss tensor Parameters ---------- yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body y_true: list of array, the output of preprocess_true_boxes anchors: array, shape=(N, 2), wh num_classes: integer ignore_thresh: float, the iou threshold whether to ignore object confidence loss Returns ------- loss: tensor, shape=(1,) &#x27;&#x27;&#x27; num_layers = len(anchors)//3 # default setting #将预测结果和实际ground truth分开，args是[*model_body.output,*y_true] #y_true是一个列表，包含三个特征层，shape分别为(m,13,13,3,85),(m,26,26,3,85),(m,52,52,3,85) #yolo_outputs是一个列表，包含三个特征层，shape分别为(m,13,13,3,85),(m,26,26,3,85),(m,52,52,3,85) yolo_outputs = args[:num_layers] y_true = args[num_layers:] anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0])) grid_shapes = [K.cast(K.shape(yolo_outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)] loss = 0 m = K.shape(yolo_outputs[0])[0] # batch size, tensor mf = K.cast(m, K.dtype(yolo_outputs[0])) for l in range(num_layers): #取出其对应的种类(m,X,X,3,1) object_mask = y_true[l][..., 4:5]#obiect_mask就是置信度 #取出其对应的种类(m,X,X,3,80) true_class_probs = y_true[l][..., 5:] #将yolo_outputs的特征层输出进行处理 #grid为网格结构(x,x,1,2),raw_pred为尚未处理的预测结果(m,x,x,3,85) #还有解码后的xy，wh，(m,13,13,3,2) grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l], anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True) #解码后的预测的box的位置 #(m,13,13,3,4) pred_box = K.concatenate([pred_xy, pred_wh]) # Darknet raw box to calculate loss. raw_true_xy = y_true[l][..., :2]*grid_shapes[l][::-1] - grid raw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1]) raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) # avoid log(0)=-inf#switch接口，就是一个if/else条件判断语句 box_loss_scale = 2 - y_true[l][...,2:3]*y_true[l][...,3:4] # Find ignore mask, iterate over each of batch. ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True) object_mask_bool = K.cast(object_mask, &#x27;bool&#x27;) def loop_body(b, ignore_mask): true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0]) #计算预测结果与真实情况的iou #pred_box为13，13，3，4 #计算的结果是每个pred_box和其它所有真实框的iou #13，13，3，n iou = box_iou(pred_box[b], true_box) best_iou = K.max(iou, axis=-1) ignore_mask = ignore_mask.write(b, K.cast(best_iou&lt;ignore_thresh, K.dtype(true_box))) return b+1, ignore_mask #遍历所有图片 _, ignore_mask = K.control_flow_ops.while_loop(lambda b,*args: b&lt;m, loop_body, [0, ignore_mask])#K.control_flow_ops.while_loop(cond, body, loop_vars)，其中cond是条件判断函数，返回True或False; body是循环体；loop_vars是传入cond和body的参数列表。 #将每幅图的内容压缩，进行处理 ignore_mask = ignore_mask.stack() #(m,13,13,3,1,1) ignore_mask = K.expand_dims(ignore_mask, -1) # K.binary_crossentropy is helpful to avoid exp overflow. xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,0:2], from_logits=True) wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh-raw_pred[...,2:4]) #如果该位置本来有框，那么计算1与置信度的交叉熵 #如果该位置本来没有框，而且满足best_iou&lt;ignore_thresh，则被认定为负样本 #best_iou&lt;ignore_thresh用于限制负样本 confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) (1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_mask class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True) xy_loss = K.sum(xy_loss) / mf wh_loss = K.sum(wh_loss) / mf confidence_loss = K.sum(confidence_loss) / mf class_loss = K.sum(class_loss) / mf loss += xy_loss + wh_loss + confidence_loss + class_loss if print_loss: loss = tf.Print(loss, [loss, xy_loss, wh_loss, confidence_loss, class_loss, K.sum(ignore_mask)], message=&#x27;loss: &#x27;) return loss 在loss值的计算过程中预测框被分为三类：正例：任取一个ground truth，与4032个框全部计算IOU，IOU最大的预测框，即为正例。并且一个预测框，只能分配给一个ground truth。例如第一个ground truth已经匹配了一个正例检测框，那么下一个ground truth，就在余下的4031个检测框中，寻找IOU最大的检测框作为正例。ground truth的先后顺序可忽略。正例产生置信度loss、检测框loss、类别loss。预测框为对应的ground truth box标签（需要反向编码，使用真实的x、y、w、h计算出 [公式] ）；类别标签对应类别为1，其余为0；置信度标签为1。忽略样例：正例除外，与任意一个ground truth的IOU大于阈值（论文中使用0.5），则为忽略样例。忽略样例不产生任何loss。（存在意义：由于Yolov3使用了多尺度特征图，不同尺度的特征图之间会有重合检测部分。比如有一个真实物体，在训练时被分配到的检测框是特征图1的第三个box，IOU达0.98，此时恰好特征图2的第一个box与该ground truth的IOU达0.95，也检测到了该ground truth，如果此时给其置信度强行打0的标签，网络学习效果会不理想。）负例：正例除外（与ground truth计算后IOU最大的检测框，但是IOU小于阈值，仍为正例），与全部ground truth的IOU都小于阈值（0.5），则为负例。负例只有置信度产生loss，置信度标签为0。&gt; 图片预测1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def yolo_eval(yolo_outputs, anchors, num_classes, image_shape, max_boxes=20, score_threshold=.6, iou_threshold=.5): &quot;&quot;&quot;Evaluate YOLO model on given input and return filtered boxes.&quot;&quot;&quot; #获得特征层的数量 num_layers = len(yolo_outputs) #特征层1对应的anchor是678 #特征层2对应的anchor是345 #特征层3对应的anchor是012 anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default setting input_shape = K.shape(yolo_outputs[0])[1:3] * 32 boxes = [] box_scores = [] #对每个特征层进行处理 for l in range(num_layers): _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l], anchors[anchor_mask[l]], num_classes, input_shape, image_shape) boxes.append(_boxes) box_scores.append(_box_scores) #将每个特征层结果进行堆叠 boxes = K.concatenate(boxes, axis=0) box_scores = K.concatenate(box_scores, axis=0) mask = box_scores &gt;= score_threshold max_boxes_tensor = K.constant(max_boxes, dtype=&#x27;int32&#x27;) boxes_ = [] scores_ = [] classes_ = [] for c in range(num_classes): # TODO: use keras backend instead of tf. #取出所有box_scores&gt;=score_threshold的框和成绩 class_boxes = tf.boolean_mask(boxes, mask[:, c])#tf.boolean_mask 的作用是 通过布尔值 过滤元素tensor：被过滤的元素mask：一堆 bool 值，它的维度不一定等于 tensorreturn： mask 为 true 对应的 tensor 的元素当 tensor 与 mask 维度一致时，return 一维 class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c]) #非极大抑制 nms_index = tf.image.non_max_suppression( class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold) #获得非极大值抑制后的结果 #下列三个分别是：框的位置，得分与种类 class_boxes = K.gather(class_boxes, nms_index) class_box_scores = K.gather(class_box_scores, nms_index)#gather（reference,indices)在给定的张量中搜索给定下标的向量。 classes = K.ones_like(class_box_scores, &#x27;int32&#x27;) * c#ones_like给定一个tensor（tensor 参数），该操作返回一个具有和给定tensor相同形状（shape）和相同数据类型（dtype），但是所有的元素都被设置为1的tensor。 boxes_.append(class_boxes) scores_.append(class_box_scores) classes_.append(classes) boxes_ = K.concatenate(boxes_, axis=0) scores_ = K.concatenate(scores_, axis=0) classes_ = K.concatenate(classes_, axis=0) return boxes_, scores_, classes_","categories":[{"name":"Yolo学习","slug":"Yolo学习","permalink":"http://example.com/categories/Yolo%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"yolov2做出的改进","slug":"Yolo学习/yolov2做出的改进","date":"2025-08-15T08:34:27.000Z","updated":"2025-08-24T20:16:56.000Z","comments":true,"path":"wiki/Yolo学习/yolov2做出的改进/","permalink":"http://example.com/wiki/Yolo%E5%AD%A6%E4%B9%A0/yolov2%E5%81%9A%E5%87%BA%E7%9A%84%E6%94%B9%E8%BF%9B/","excerpt":"","text":"yolov1的缺点YOLOv1虽然检测速度很快，但是在检测精度上却不如R-CNN系检测方法，YOLOv1在物体定位方面（localization）不够准确，并且召回率（recall）较低。 解决方法1.Batch NormalizaBatch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。 2.High Resolution Classifier用更高分辨率的输入（448x448）之前是（224x224）。 3.Convolutional With Anchor Boxes以前 yolo1存在的问题在YOLOv1中，输入图片最终被划分为$7\\times 7$ 网格，每个单元格预测2个边界框。YOLOv1最后采用的是全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比（scales and ratios)的物体，YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差。 改进思路用anchor boxes（先验框） 采用先验框使得模型更容易学习。所以YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。 效果使用anchor boxes之后，YOLOv2的mAP有稍微下降（这里下降的原因，我猜想是YOLOv2虽然使用了anchor boxes，但是依然采用YOLOv1的训练方法）。YOLOv1只能预测98个边界框（ ），而YOLOv2使用anchor boxes之后可以预测上千个边界框（ )。所以使用anchor boxes之后，YOLOv2的召回率大大提升，由原来的81%升至88%。 4.Dimension Clusters先验框的维度（长和宽）都是手动设定的，带有一定的主观性。如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。 聚类分析怎么做的我还没有看 因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标： 5.New Network: Darknet-19YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如图4所示。Darknet-19与VGG16模型设计原则是一致的，主要采用 卷积，采用 的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用global avgpooling做预测，并且在 卷积之间使用 卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。 性能提升在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。 6.Direct location prediction前面讲到，YOLOv2借鉴RPN网络使用anchor boxes来预测边界框相对先验框的offsets。边界框的实际中心位置 ，需要根据预测的坐标偏移值 ，先验框的尺度 以及中心坐标 （特征图每个位置的中心点）来计算： 但是上面的公式是无约束的，预测的边界框很容易向任何方向偏移，如当 时边界框将向右偏移先验框的一个宽度大小，而当 时边界框将向左偏移先验框的一个宽度大小，因此每个位置预测的边界框可以落在图片任何位置，这导致模型的不稳定性，在训练时需要很长时间来预测出正确的offsets。 所以，YOLOv2弃用了这种预测方式，而是沿用YOLOv1的方法，就是预测边界框中心点相对于对应cell左上角位置的相对偏移值，为了将边界框中心点约束在当前cell中，使用sigmoid函数处理偏移值，这样预测的偏移值在(0,1)范围内（每个cell的尺度看做1）。总结来看，根据边界框预测的4个offsets ，可以按如下公式计算出边界框实际位置和大小： 其中 为cell的左上角坐标，如图5所示，在计算时每个cell的尺度为1，所以当前cell的左上角坐标为 。由于sigmoid函数的处理，边界框的中心位置会约束在当前cell内部，防止偏移过多。而 和 是先验框的宽度与长度，前面说过它们的值也是相对于特征图大小的，在特征图中每个cell的长和宽均为1。这里记特征图的大小为 （在文中是 )，这样我们可以将边界框相对于整张图片的位置和大小计算出来（4个值均在0和1之间）： 如果再将上面的4个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的最终位置和大小了。这就是YOLOv2边界框的整个解码过程。约束了边界框的位置预测值使得模型更容易稳定训练，结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。 7.Fine-Grained FeaturesYOLOv2的输入图片大小为$416\\times 416$ ，经过5次maxpooling之后得到$13\\times 13$大小的特征图，并以此特征图采用卷积做预测。 大小的特征图对检测大物体是足够了，但是对于小物体还需要更精细的特征图（Fine-Grained Features)。 YOLOv2所利用的Fine-Grained Features是 大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为 的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个 的局部区域，然后将其转化为channel维度，对于 的特征图，经passthrough层处理之后就变成了 的新特征图（特征图大小降低4倍，而channles增加4倍，图6为一个实例），这样就可以与后面的 特征图连接在一起形成 大小的特征图，然后在此特征图基础上卷积做预测。 另外，作者在后期的实现中借鉴了ResNet网络，不是直接对高分辨特征图处理，而是增加了一个中间卷积层，先采用64个 卷积核进行卷积，然后再进行passthrough处理，这样 的特征图得到 的特征图。这算是实现上的一个小细节。使用Fine-Grained Features之后YOLOv2的性能有1%的提升。 8.Multi-Scale Training由于YOLOv2模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于 大小的图片。为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。 由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值： ，输入图片最小为 ，此时对应的特征图大小为 （不是奇数了，确实有点尴尬），而输入图片最大为 ，对应的特征图大小为 。在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。","categories":[{"name":"Yolo学习","slug":"Yolo学习","permalink":"http://example.com/categories/Yolo%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"yolo1 思想与输出的形式","slug":"Yolo学习/yolo1 思想与输出的形式","date":"2025-08-13T10:34:27.000Z","updated":"2025-08-24T20:06:06.000Z","comments":true,"path":"wiki/Yolo学习/yolo1 思想与输出的形式/","permalink":"http://example.com/wiki/Yolo%E5%AD%A6%E4%B9%A0/yolo1%20%E6%80%9D%E6%83%B3%E4%B8%8E%E8%BE%93%E5%87%BA%E7%9A%84%E5%BD%A2%E5%BC%8F/","excerpt":"","text":"只说结论yolo1 将一张图片分割为S*S个单元格，每个单元格输出（Bx5+C）个值。 解释： B：每个单元格将会预测B个边框。（就是圈B个框框） 5：每个圈出来的框框有5个参数 ，其中前四个是相对于单元格的比例（就是0到1之间的数)。c代表该单元格的置信度。用该单元格含有目标的概率与准确度的乘积表示。 C: one-hot编码的属于C个类别的预测值。 所有单元格的就是S*S（Bx5+C）个值 非极大值抑制算法（non maximum suppression, NMS）这个算法不单单是针对Yolo算法的，而是所有的检测算法中都会用到。NMS算法主要解决的是一个目标被多次检测的问题，如人脸检测，可以看到人脸被多次检测，但是其实我们希望最后仅仅输出其中一个最好的预测框。 首先从所有的检测框中找到置信度最大的那个框，然后挨个计算其与剩余框的IOU（个人理解为重合区域比例），如果其值大于一定阈值（重合度过高），那么就将该框剔除；然后对剩余的检测框重复上述过程，直到处理完所有的检测框。Yolo预测过程也需要用到NMS算法。 滑动窗口与CNN滑动窗口介绍采用滑动窗口的目标检测算法思路非常简单，它将检测问题转化为了图像分类问题。其基本原理就是采用不同大小和比例（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类，这样就可以实现对整张图片的检测了 缺点但是这个方法有致命的缺点，就是你并不知道要检测的目标大小是什么规模，所以你要设置不同大小和比例的窗口去滑动，而且还要选取合适的步长。这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量，所以你的分类器不能太复杂，因为要保证速度。 CNN可以使用CNN实现更高效的滑动窗口方法 一种全卷积的方法，简单来说就是网络中用卷积层代替了全连接层 输入图片大小是16x16，经过一系列卷积操作，提取了2x2的特征图，但是这个2x2的图上每个元素都是和原图是一一对应的，这不就是相当于在原图上做大小为14x14的窗口滑动，且步长为2，共产生4个子区域。 之所CNN可以实现这样的效果是因为卷积操作的特性，就是图片的空间位置信息的不变性，尽管卷积过程中图片大小减少，但是位置对应关系还是保存的。 缺点尽管可以减少滑动窗口的计算量，但是只是针对一个固定大小与步长的窗口 yolo如何做的思想直接将原始图片分割成互不重合的小方块，然后通过卷积最后生产这样大小的特征图。可以认为特征图的每个元素也是对应原始图片的一个小方块，然后用每个元素来可以预测那些中心点在该小方格内的目标。 具体点的具体来说，Yolo的CNN网络将输入的图片分割成S*S个单元格，然后每个单元格负责去检测那些中心点落在该格子内的目标 每个单元格会预测 B个边界框（bounding box）以及边界框的置信度（confidence score) 置信度所谓置信度其实包含两个方面，一是这个边界框含有目标的可能性大小，二是这个边界框的准确度。 含有目标的可能性大小记为 ，当该边界框是背景时（即不包含目标），此时 。而当该边界框包含目标时， 边界框的准确度可以用预测框与实际框（ground truth）的IOU（intersection over union，交并比）来表征，记为 。 因此置信度可以定义为 输出解释边界框的大小与位置可以用4个值来表征： ，其中 是边界框的中心坐标，而 和 是边界框的宽与高 还有一点要注意，中心坐标的预测值 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的，单元格的坐标定义如图6所示。而边界框的 和 预测值是相对于整个图片的宽与高的比例，这样4个元素的大小应该在 范围。 总输出一部分每个边界框的预测值实际上包含5个元素： ，其中前4个表征边界框的大小与位置，而最后一个值是置信度。 另一部分（分类问题）每一个单元格其还要给出预测出 个类别概率值，但yolo1每个单元格只预测一个类别。后来的版本会有改进，暂时不提。 所以一个单元格会有个值 总的S*S个会有 个值","categories":[{"name":"Yolo学习","slug":"Yolo学习","permalink":"http://example.com/categories/Yolo%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"yolov1 具体网络结构","slug":"Yolo学习/yolo1 具体网络结构","date":"2025-08-12T08:34:27.000Z","updated":"2025-08-24T18:53:10.000Z","comments":true,"path":"wiki/Yolo学习/yolo1 具体网络结构/","permalink":"http://example.com/wiki/Yolo%E5%AD%A6%E4%B9%A0/yolo1%20%E5%85%B7%E4%BD%93%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/","excerpt":"","text":"大体结构Yolo采用卷积网络来提取特征，然后使用全连接层来得到预测值。 卷积层：主要使用1x1卷积来做channle reduction，然后紧跟3x3卷积。 卷积层和全连接层，采用Leaky ReLU激活函数： 但是最后一层却采用线性激活函数。 直接上图 这里我没看懂最后一个为什么从一条又变成了一块。 输出的具体解释 关于训练Yolo算法将目标检测看成回归问题，所以采用的是均方差损失函数。 权重部分对不同的部分采用了不同的权重值。 首先区分定位误差和分类误差。 定位误差即边界框坐标预测误差，采用较大的权重 。 分类误差然后其区分不包含目标的边界框与含有目标的边界框的置信度，对于前者，采用较小的权重值 ，其它权重值均设为1。 误差部分采用均方误差，其同等对待大小不同的边界框， 但是实际上较小的边界框的坐标误差应该要比较大的边界框要更敏感。为了保证这一点，将网络的边界框的宽与高预测改为对其平方根的预测，即预测值变为 。 另外由于每个单元格预测多个边界框。但是其对应类别只有一个。那么在训练时，如果该单元格内确实存在目标，那么只选择与ground truth的IOU最大的那个边界框来负责预测该目标，而其它边界框认为不存在目标。 好处这样设置的一个结果将会使一个单元格对应的边界框更加专业化，其可以分别适用不同大小，不同高宽比的目标，从而提升模型性能。 缺点如果一个单元格内存在多个目标怎么办，其实这时候Yolo算法就只能选择其中一个来训练 注意：对于不存在对应目标的边界框，其误差项就是只有置信度，坐标项误差是没法计算的。而只有当一个单元格内确实存在目标时，才计算分类误差项，否则该项也是无法计算的。 最终的损失函数计算如下： 关于预测这里我们不考虑batch，认为只是预测一张输入图片。根据前面的分析，最终的网络输出是 ，但是我们可以将其分割成三个部分 类别概率部分为 置信度部分为 置信度部分为 然后将前两项相乘可以得到类别置信度值为 这里总共预测了 个边界框。 有两种处理方法第一种策略首先，对于每个预测框根据类别置信度选取置信度最大的那个类别作为其预测标签 经过这层处理我们得到各个预测框的预测类别及对应的置信度值，其大小都是 。 一般情况下，会设置置信度阈值，就是将置信度小于该阈值的box过滤掉，所以经过这层处理，剩余的是置信度比较高的预测框。 最后再对这些预测框使用NMS算法。 第二种策略先使用NMS，然后再确定各个box的类别， 对于98个boxes，首先将小于置信度阈值的值归0，然后分类别地对置信度值采用NMS，这里NMS处理结果不是剔除，而是将其置信度值归为0。最后才是确定各个box的类别，当其置信度值不为0时才做出检测结果输出。 我读到的文章说：这个策略不是很直接，但是貌似Yolo源码就是这样做的。Yolo论文里面说NMS算法对Yolo的性能是影响很大的，所以可能这种策略对Yolo更好。但是我测试了普通的图片检测，两种策略结果是一样的。","categories":[{"name":"Yolo学习","slug":"Yolo学习","permalink":"http://example.com/categories/Yolo%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"anchor box（先验框）","slug":"Yolo学习/anchor boxes","date":"2025-08-10T08:34:27.000Z","updated":"2025-08-24T18:32:30.000Z","comments":true,"path":"wiki/Yolo学习/anchor boxes/","permalink":"http://example.com/wiki/Yolo%E5%AD%A6%E4%B9%A0/anchor%20boxes/","excerpt":"","text":"anchor box（先验框）anchor （锚; 给以安全感的人(或物); 精神支柱; 顶梁柱;） 为什么对象检测中存在一个问题就是每个格子只能预测一个对象，如果想让一个格子检测出多个对象，用anchor box 以前的做法假设我们有这样一张图片，对于这个例子，我们使用3x3的网格，可以观察到，行人和汽车的中心几乎在同一个网格，然而我们以前的方法一个格子只能预测一个对象，而且对于y输出的的向量量𝑦 &#x3D; [𝑝𝑐 𝑏𝑥 𝑏𝑦 𝑏ℎ 𝑏𝑤 𝑐1 𝑐2 𝑐3 ]，你可以检测这三个类别，行人、汽车和摩托车，它将无法输出检测结果，所以我必须从两个检测结果中选一个，这便影响了模型性能，导致一些对象被丢弃无法检测出来。 anchor box 引入 预先定义两个不同形状的 anchor box，或者 anchor box 形状 一般来说，可能会用更多的 anchor box，可能要 5 个甚至更多，为介绍方便就用两个 anchor box 定义类别标签，用的向量不再是下面这个：[𝑝𝑐 𝑏𝑥 𝑏𝑦 𝑏ℎ 𝑏𝑤 𝑐1 𝑐2 𝑐3]𝑇 而是输出重复两次： 𝑦 &#x3D; [𝑝𝑐 𝑏𝑥 𝑏𝑦 𝑏ℎ 𝑏𝑤 𝑐1 𝑐2 𝑐3 𝑝𝑐 𝑏𝑥 𝑏𝑦 𝑏ℎ 𝑏𝑤 𝑐1 𝑐2 𝑐3]𝑇 前一半是box1后一半是box2 行人一般符合anchor box1形状，所以用anchor box1来预测行人会达到很好的效果，这么编码𝑝𝑐 &#x3D; 1，代表有个行人，用𝑏𝑥,𝑏𝑦,𝑏ℎ和𝑏𝑤来编码包住行人的边界框，然后用𝑐1,𝑐2,𝑐3(𝑐1 &#x3D; 1,𝑐2 &#x3D; 0,𝑐3 &#x3D; 0)来说明这个对象是个行人。汽车一般是box2，方式同理 文章没说编码完一部分后剩下的怎么处理 总结所以，总的来说，anchor box是这么来做的，现在每个对象和以前一样根据中心点分配到一个格子中，然后看和每个anchor box的IoU（交并比），选择IoU最高的那个，用这个anchor box来进行预测。输出y的维度是nxnxmxc（n为图片分成nxn份，m为anchor box数量，c为class类别数） 不足1.两个 anchor box，但在同一个格子中有三个对象，这种情况算法处理不好，你希望这种情况不会发生，但如果真的发生了，这个算法并没有很好的处理办法 2.两个对象都分配到一个格子中，而且它们的 anchor box 形状也一样，也不好办 看文章说其实这两种情况很少出现，特别是如果你用的是 19×19 网格而不是3×3 的网格 怎么选box？1.一般手工指定anchor box形状，根据要检测的对象，指定有针对性地anchor box，可选择5-10个anchor box，使其尽可能覆盖到不同形状。2.使用K-means聚类算法获得anchor box。","categories":[{"name":"Yolo学习","slug":"Yolo学习","permalink":"http://example.com/categories/Yolo%E5%AD%A6%E4%B9%A0/"}],"tags":[]}],"categories":[{"name":"Yolo学习","slug":"Yolo学习","permalink":"http://example.com/categories/Yolo%E5%AD%A6%E4%B9%A0/"}],"tags":[]}